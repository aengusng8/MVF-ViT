# -*- coding: utf-8 -*-
"""Local_Block.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTmwj8GkaiJOU7WaAlcvPFrEDX5JK2o5

Phần 1 : Ghi chép lại một số lý thuyết về Pytorch 2.0 ( không cần đọc )

Bài 1 : Giải thích về các package của torch
torch: the top-level package containing PyTorch's entire ecosystem and Tensor library.

torch.nn: a sub-library containing the building blocks for building neural networks, such as layers, activation functions, loss functions, etc.

torch.optim: a sub-library containing optimization algorithms like stochastic gradient descent, Adam, etc.

torch.autograd: a sub-library for automatic differentiation (i.e., backpropagation), which allows you to compute gradients of your tensors' values with respect to some function's output.

torch.utils: a sub-library containing utility functions like data loaders for common datasets, random number generation, and more.

torchvision: a package containing popular datasets, model architectures, and image processing tools for computer vision tasks.

torchtext: a package containing datasets and tools for natural language processing.

torchaudio: a package for processing audio signals.

torch.distributions: a sub-library containing probability distributions for use in probabilistic programming.

torch.onnx: a sub-library for exporting models to the Open Neural Network Exchange (ONNX) format.

torch.cuda: a sub-library providing CUDA support for PyTorch on NVIDIA GPUs.

torch.quantization: a sub-library for model quantization (i.e., reducing the precision of model parameters and activations to save memory and computation).

torch.jit: a sub-library for just-in-time (JIT) compilation of PyTorch models for faster execution.

torch.utils.data: a sub-library containing tools for loading and processing data in parallel, such as data loaders and samplers.

Bài 2 : nn.Module 

nn.Module là module chứa hầu hết tất cả các layer neural network như nn.Linear, nn.Conv2d, nn.LSTM v.v., đều là con của nn.Module và kế thừa tất cả các tính năng của nó

Người dùng có thể tự định nghĩa mô hình training custom của mình bằng việc thừa kế từ nn.Module , và ghi đè lên các phương thức __init__ và forward của nó

Bài 3 : Giải thích về nn.functional
nn.functional module in PyTorch contains many commonly used functions in building neural networks. Here are some of the functions:

Activation functions:

relu,
sigmoid,
tanh,
softmax,
gelu,
Convolutional functions:

conv1d,
conv2d,
conv3d,
Pooling functions:

max_pool1d,
max_pool2d,
max_pool3d,
avg_pool1d,
avg_pool2d,
avg_pool3d,
Normalization functions:

batch_norm1d,
batch_norm2d,
batch_norm3d,
instance_norm1d,
instance_norm2d,
instance_norm3d,
Loss functions:

binary_cross_entropy,
mse_loss,
nll_loss,
cross_entropy,
l1_loss,
poisson_nll_loss,
kl_div,
Other functions:

dropout,
linear,
embedding,
pad,
one_hot,
interpolate,
upsample
"""

!pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117

"""Đây là classifier"""

import torch
import torch.nn as nn

class LocalClassifier(nn.Module):
    def __init__(self, token_dim):
        super(LocalClassifier, self).__init__()
        self.left_eye_layer = nn.Sequential(
            nn.Linear(token_dim, token_dim // 2),
            nn.BatchNorm1d(token_dim // 2),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(token_dim // 2 , token_dim // 4),
            nn.BatchNorm1d(token_dim // 4),
            nn.Dropout(0.5),
            nn.ReLU()
        )
        self.right_eye_layer = nn.Sequential(
            nn.Linear(token_dim, token_dim // 2),
            nn.BatchNorm1d(token_dim // 2),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(token_dim // 2 , token_dim // 4),
            nn.BatchNorm1d(token_dim // 4),
            nn.Dropout(0.5),
            nn.ReLU()
        )
        self.nose_layer = nn.Sequential(
            nn.Linear(token_dim, token_dim // 2),
            nn.BatchNorm1d(token_dim // 2),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(token_dim // 2 , token_dim // 4),
            nn.BatchNorm1d(token_dim // 4),
            nn.Dropout(0.5),
            nn.ReLU()
        )
        self.mouth_layer = nn.Sequential(
            nn.Linear(token_dim, token_dim // 2),
            nn.BatchNorm1d(token_dim // 2),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(token_dim // 2 , token_dim // 4),
            nn.BatchNorm1d(token_dim // 4),
            nn.Dropout(0.5),
            nn.ReLU()
        )
        self.output_layer = nn.Linear(token_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, left_eye_input, right_eye_input, nose_input, mouth_input):
        left_eye_layer = self.left_eye_layer(left_eye_input)
        right_eye_layer = self.right_eye_layer(right_eye_input)
        nose_layer = self.nose_layer(nose_input)
        mouth_layer = self.mouth_layer(mouth_input)
        concatenated = torch.cat((left_eye_layer, right_eye_layer, nose_layer, mouth_layer), dim=1)
        # concatenated = concatenated.flatten()
        # print("shape of concat : " + str(concatenated.shape))
        output = self.output_layer(concatenated)
        output = self.sigmoid(output)
        return output

D = 1024
model = LocalClassifier(D)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

"""Đây là gen ra Q-local"""

import torch
import torch.nn as nn

class Qlocal(nn.Module):
    def __init__(self, token_dim = 1024):
        super(Qlocal, self).__init__()
        self.token_dim = token_dim
        self.conv = nn.Sequential(
            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )
        self.fc = nn.Linear(in_features=4*token_dim, out_features=16*token_dim)
        
    def forward(self, x):
        x = torch.cat(x, dim=0).view(4, self.token_dim)
        x = self.conv(x.unsqueeze(0))
        x = x.view(-1, 4*self.token_dim)
        x = self.fc(x)
        x = x.view(-1, self.token_dim)
        return x
token_dim = 1024
Q_gen = Qlocal(token_dim)

# Create input tensors
t1 = torch.randn(token_dim)
t2 = torch.randn(token_dim)
t3 = torch.randn(token_dim)
t4 = torch.randn(token_dim)

# Extract features
Q = Q_gen([t1, t2, t3, t4])
print(Q.shape) # Should output (16, 1024)

"""If you want to run test for the classifier :"""

# Generate some sample data
N = 1000
D = 1024
x1 = torch.randn(N, D)
x2 = torch.randn(N, D)
x3 = torch.randn(N, D)
x4 = torch.randn(N, D)
y = torch.randint(0, 2, (N, 1)).float()

# Split the data into train and test sets
x1_train, x1_test = x1[:800], x1[800:]
x2_train, x2_test = x2[:800], x2[800:]
x3_train, x3_test = x3[:800], x3[800:]
x4_train, x4_test = x4[:800], x4[800:]
y_train, y_test = y[:800], y[800:]

# Train the model
epochs = 100
for epoch in range(epochs):
    optimizer.zero_grad()
    y_pred = model(x1_train, x2_train, x3_train, x4_train)
    loss = criterion(y_pred, y_train)
    loss.backward()
    optimizer.step()

# Evaluate the model
with torch.no_grad():
    y_pred = model(x1_test, x2_test, x3_test, x4_test)
    loss = criterion(y_pred, y_test)
    accuracy = ((y_pred > 0.5) == y_test).sum().item() / len(y_test)

print('Accuracy:', accuracy)