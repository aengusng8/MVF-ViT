{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Phần 1 : Ghi chép lại một số lý thuyết về Pytorch 2.0 ( không cần đọc )\n",
        "\n",
        "Bài 1 : Giải thích về các package của torch\n",
        "torch: the top-level package containing PyTorch's entire ecosystem and Tensor library.\n",
        "\n",
        "torch.nn: a sub-library containing the building blocks for building neural networks, such as layers, activation functions, loss functions, etc.\n",
        "\n",
        "torch.optim: a sub-library containing optimization algorithms like stochastic gradient descent, Adam, etc.\n",
        "\n",
        "torch.autograd: a sub-library for automatic differentiation (i.e., backpropagation), which allows you to compute gradients of your tensors' values with respect to some function's output.\n",
        "\n",
        "torch.utils: a sub-library containing utility functions like data loaders for common datasets, random number generation, and more.\n",
        "\n",
        "torchvision: a package containing popular datasets, model architectures, and image processing tools for computer vision tasks.\n",
        "\n",
        "torchtext: a package containing datasets and tools for natural language processing.\n",
        "\n",
        "torchaudio: a package for processing audio signals.\n",
        "\n",
        "torch.distributions: a sub-library containing probability distributions for use in probabilistic programming.\n",
        "\n",
        "torch.onnx: a sub-library for exporting models to the Open Neural Network Exchange (ONNX) format.\n",
        "\n",
        "torch.cuda: a sub-library providing CUDA support for PyTorch on NVIDIA GPUs.\n",
        "\n",
        "torch.quantization: a sub-library for model quantization (i.e., reducing the precision of model parameters and activations to save memory and computation).\n",
        "\n",
        "torch.jit: a sub-library for just-in-time (JIT) compilation of PyTorch models for faster execution.\n",
        "\n",
        "torch.utils.data: a sub-library containing tools for loading and processing data in parallel, such as data loaders and samplers."
      ],
      "metadata": {
        "id": "uoCfkMMzXFnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 2 : nn.Module \n",
        "\n",
        "nn.Module là module chứa hầu hết tất cả các layer neural network như nn.Linear, nn.Conv2d, nn.LSTM v.v., đều là con của nn.Module và kế thừa tất cả các tính năng của nó\n",
        "\n",
        "Người dùng có thể tự định nghĩa mô hình training custom của mình bằng việc thừa kế từ nn.Module , và ghi đè lên các phương thức __init__ và forward của nó "
      ],
      "metadata": {
        "id": "WWlWAe-ZZB2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bài 3 : Giải thích về nn.functional\n",
        "nn.functional module in PyTorch contains many commonly used functions in building neural networks. Here are some of the functions:\n",
        "\n",
        "Activation functions:\n",
        "\n",
        "relu,\n",
        "sigmoid,\n",
        "tanh,\n",
        "softmax,\n",
        "gelu,\n",
        "Convolutional functions:\n",
        "\n",
        "conv1d,\n",
        "conv2d,\n",
        "conv3d,\n",
        "Pooling functions:\n",
        "\n",
        "max_pool1d,\n",
        "max_pool2d,\n",
        "max_pool3d,\n",
        "avg_pool1d,\n",
        "avg_pool2d,\n",
        "avg_pool3d,\n",
        "Normalization functions:\n",
        "\n",
        "batch_norm1d,\n",
        "batch_norm2d,\n",
        "batch_norm3d,\n",
        "instance_norm1d,\n",
        "instance_norm2d,\n",
        "instance_norm3d,\n",
        "Loss functions:\n",
        "\n",
        "binary_cross_entropy,\n",
        "mse_loss,\n",
        "nll_loss,\n",
        "cross_entropy,\n",
        "l1_loss,\n",
        "poisson_nll_loss,\n",
        "kl_div,\n",
        "Other functions:\n",
        "\n",
        "dropout,\n",
        "linear,\n",
        "embedding,\n",
        "pad,\n",
        "one_hot,\n",
        "interpolate,\n",
        "upsample"
      ],
      "metadata": {
        "id": "yfKPEDnvb3RZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd9pSh9SsgvR",
        "outputId": "4ed9af1b-585d-4104-e6bf-888be773c3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu117, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/numpy-1.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu117/torch-2.1.0.dev20230430%2Bcu117-cp310-cp310-linux_x86_64.whl (1845.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m797.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading https://download.pytorch.org/whl/nightly/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting fsspec\n",
            "  Downloading https://download.pytorch.org/whl/nightly/fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading https://download.pytorch.org/whl/nightly/networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==2.1.0+7d1a95b046\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-2.1.0%2B7d1a95b046-cp310-cp310-linux_x86_64.whl (88.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading https://download.pytorch.org/whl/nightly/Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading https://download.pytorch.org/whl/nightly/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading https://download.pytorch.org/whl/nightly/MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, numpy, networkx, MarkupSafe, fsspec, filelock, pytorch-triton, jinja2, torch\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.2\n",
            "    Uninstalling MarkupSafe-2.1.2:\n",
            "      Successfully uninstalled MarkupSafe-2.1.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.12.0\n",
            "    Uninstalling filelock-3.12.0:\n",
            "      Successfully uninstalled filelock-3.12.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.1.0.dev20230430+cu117 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 2.1.0.dev20230430+cu117 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.0.dev20230430+cu117 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.1.0.dev20230430+cu117 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\n",
            "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0.dev20230430+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 filelock-3.9.0 fsspec-2023.4.0 jinja2-3.1.2 mpmath-1.2.1 networkx-3.0rc1 numpy-1.24.1 pytorch-triton-2.1.0+7d1a95b046 sympy-1.11.1 torch-2.1.0.dev20230430+cu117 typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là classifier"
      ],
      "metadata": {
        "id": "UFQnsVEHRLGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LocalClassifier(nn.Module):\n",
        "    def __init__(self, token_dim):\n",
        "        super(LocalClassifier, self).__init__()\n",
        "        self.left_eye_layer = nn.Sequential(\n",
        "            nn.Linear(token_dim, token_dim // 2),\n",
        "            nn.BatchNorm1d(token_dim // 2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(token_dim // 2 , token_dim // 4),\n",
        "            nn.BatchNorm1d(token_dim // 4),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.right_eye_layer = nn.Sequential(\n",
        "            nn.Linear(token_dim, token_dim // 2),\n",
        "            nn.BatchNorm1d(token_dim // 2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(token_dim // 2 , token_dim // 4),\n",
        "            nn.BatchNorm1d(token_dim // 4),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.nose_layer = nn.Sequential(\n",
        "            nn.Linear(token_dim, token_dim // 2),\n",
        "            nn.BatchNorm1d(token_dim // 2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(token_dim // 2 , token_dim // 4),\n",
        "            nn.BatchNorm1d(token_dim // 4),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mouth_layer = nn.Sequential(\n",
        "            nn.Linear(token_dim, token_dim // 2),\n",
        "            nn.BatchNorm1d(token_dim // 2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(token_dim // 2 , token_dim // 4),\n",
        "            nn.BatchNorm1d(token_dim // 4),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output_layer = nn.Linear(token_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, left_eye_input, right_eye_input, nose_input, mouth_input):\n",
        "        left_eye_layer = self.left_eye_layer(left_eye_input)\n",
        "        right_eye_layer = self.right_eye_layer(right_eye_input)\n",
        "        nose_layer = self.nose_layer(nose_input)\n",
        "        mouth_layer = self.mouth_layer(mouth_input)\n",
        "        concatenated = torch.cat((left_eye_layer, right_eye_layer, nose_layer, mouth_layer), dim=1)\n",
        "        # concatenated = concatenated.flatten()\n",
        "        # print(\"shape of concat : \" + str(concatenated.shape))\n",
        "        output = self.output_layer(concatenated)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "D = 1024\n",
        "model = LocalClassifier(D)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Fow9pfU4Hs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là gen ra Q-local"
      ],
      "metadata": {
        "id": "DN5aNtkERQqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, token_dim = 1024):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.token_dim = token_dim\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=4*token_dim, out_features=16*token_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.cat(x, dim=0).view(4, self.token_dim)\n",
        "        x = self.conv(x.unsqueeze(0))\n",
        "        x = x.view(-1, 4*self.token_dim)\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.token_dim)\n",
        "        return x\n",
        "token_dim = 1024\n",
        "fe = FeatureExtractor(token_dim)\n",
        "\n",
        "# Create input tensors\n",
        "t1 = torch.randn(token_dim)\n",
        "t2 = torch.randn(token_dim)\n",
        "t3 = torch.randn(token_dim)\n",
        "t4 = torch.randn(token_dim)\n",
        "\n",
        "# Extract features\n",
        "features = fe([t1, t2, t3, t4])\n",
        "print(features.shape) # Should output (16, 1024)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHucNOITRJkL",
        "outputId": "b0818a8b-7f0d-4242-f3a8-e5a16781e70e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to run test for the classifier :"
      ],
      "metadata": {
        "id": "e2fQNAELQ9fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some sample data\n",
        "N = 1000\n",
        "D = 1024\n",
        "x1 = torch.randn(N, D)\n",
        "x2 = torch.randn(N, D)\n",
        "x3 = torch.randn(N, D)\n",
        "x4 = torch.randn(N, D)\n",
        "y = torch.randint(0, 2, (N, 1)).float()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x1_train, x1_test = x1[:800], x1[800:]\n",
        "x2_train, x2_test = x2[:800], x2[800:]\n",
        "x3_train, x3_test = x3[:800], x3[800:]\n",
        "x4_train, x4_test = x4[:800], x4[800:]\n",
        "y_train, y_test = y[:800], y[800:]\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x1_train, x2_train, x3_train, x4_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    y_pred = model(x1_test, x2_test, x3_test, x4_test)\n",
        "    loss = criterion(y_pred, y_test)\n",
        "    accuracy = ((y_pred > 0.5) == y_test).sum().item() / len(y_test)\n",
        "\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cbWMMYJQ6U_",
        "outputId": "0656ad6b-b540-4394-916a-0fe1f8ffada0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.53\n"
          ]
        }
      ]
    }
  ]
}